<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>inventory optimization w/ deep q-learning | Muhammad Ryanrahmadifa (Ryan)</title> <meta name="author" content="Muhammad Ryanrahmadifa (Ryan)"> <meta name="description" content="for the love of industrial engineering"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ryanrahmadifa.github.io/blog/2023/rl-inventory-optimization/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <title>inventory optimization w/ deep q-learning | Muhammad Ryanrahmadifa (Ryan)</title> <meta name="generator" content="Jekyll v4.3.3"> <meta property="og:title" content="inventory optimization w/ deep q-learning"> <meta property="og:locale" content="en"> <meta name="description" content="for the love of industrial engineering"> <meta property="og:description" content="for the love of industrial engineering"> <link rel="canonical" href="https://ryanrahmadifa.github.io/blog/2023/rl-inventory-optimization/"> <meta property="og:url" content="https://ryanrahmadifa.github.io/blog/2023/rl-inventory-optimization/"> <meta property="og:site_name" content="Muhammad Ryanrahmadifa (Ryan)"> <meta property="og:type" content="article"> <meta property="article:published_time" content="2023-07-04T12:57:00+00:00"> <meta name="twitter:card" content="summary"> <meta property="twitter:title" content="inventory optimization w/ deep q-learning"> <script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-07-04T12:57:00+00:00","datePublished":"2023-07-04T12:57:00+00:00","description":"for the love of industrial engineering","headline":"inventory optimization w/ deep q-learning","mainEntityOfPage":{"@type":"WebPage","@id":"https://ryanrahmadifa.github.io/blog/2023/rl-inventory-optimization/"},"url":"https://ryanrahmadifa.github.io/blog/2023/rl-inventory-optimization/"}</script> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Muhammad Ryanrahmadifa (Ryan)</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">profile</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">inventory optimization w/ deep q-learning</h1> <p class="post-meta">July 4, 2023</p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/tag/reinforcement-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> reinforcement-learning</a>     ·   <a href="/blog/category/project"> <i class="fa-solid fa-tag fa-sm"></i> project</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="inventory-optimization">Inventory Optimization</h1> <p>Major inventory control policies adopted in the supply chain industry nowadays are classic static policies. A dynamic policy that can adaptively adjust the decisions of when and how much to order based on the inventory position and future demand information would be advantageous.</p> <h2 id="classic-inventory-control-policies">Classic Inventory Control Policies</h2> <ol> <li>(R,Q) Policy : R is reorder point and Q is fixed quantity of ordered product. When inventory drops below R Units, order a fixed quantity of Q units.</li> <li>(T,S) Policy: Replenish to S product units every T days, T being the review period and S as the order-up-to level.</li> <li>(R,S) Policy: When inventory drops below R Units, place an order to replenish the inventory up to S units.</li> <li>(S-1,S) policy: If there is any demand consuming the inventory on a particular day, replenish the inventory up to S units <strong>immediately</strong>.</li> </ol> <p>The four different policies metnioned above are suitable for different demand patterns, but the similarity is that they all assume either a fixed reorder point, fixed order quantity, fixed order-up-to level or fixed time interval between two orders. Moreover, most of these policies only relies on the current inventory position to make ordering decisions, and does not utilize other possible information related to future demand to help make more informed decisions.</p> <p>This limits the flexibility of the policy, which potentially undermines the responsiveness of the policy to high demand (causing lost sales) or results in excessive inventory when demand is low (causing inventory holding costs). Can we do better if we remove said limitation? Is tehre a way to build a model to obtain an inventory control policy without this limitation?</p> <p>In this project, we will try to do this with reinforcement learning (RL).</p> <h2 id="reinforcement-learning-for-inventory-optimization">Reinforcement Learning for Inventory Optimization</h2> <h3 id="formulating-the-markov-decision-process-mdp">Formulating the Markov Decision Process (MDP)</h3> <p>4 elements of MDP:</p> <ol> <li>State (S_t) -&gt; The situation of the agent at time <em>t</em> </li> <li>Action (A_t) -&gt; The decision the agent takes at time <em>t</em> </li> <li>Reward (R_t) -&gt; Feedback from the environment towards the agent’s action at time <em>t</em> </li> <li> <table> <tbody> <tr> <td>Transition Probability (S_(t+1)</td> <td>S_t, A_t) -&gt; Probability the state becomes S_(t+1) after taking a specific action A_t in the state S_t</td> </tr> </tbody> </table> </li> </ol> <p>The inventory optimization problem naturally fits the framework of MDP due to its sequential decision making structure.</p> <p>The next step is to formulate a mathematical model for the problem,</p> <p>Target Function: Maximizing the profit gotten form selling Coke within a period of time</p> <p>Parameters:</p> <ol> <li>Inventory holding cost</li> <li>Fixed ordering cost (e.g. Shipping cost)</li> <li>Variable ordering cost ( e.g. Price of buying from supplier)</li> </ol> <p>Assumptions:</p> <ol> <li>No backorder cost, assuming when there is no coke inside the store then customers would just go to another store without placing an order for the future</li> <li>Customer demand follows a mixture of normal distributions: Monday to Thursday with the lowest mean, Friday with medium mean, and Saturday to Sunday with the highest mean</li> </ol> <h3 id="rl-definitions">RL Definitions</h3> <p>We need to construct the mathematical definitions for the state, action, and reward.</p> <p>State -&gt; (I_pt, DoW_t), where I_pt is the inventory position (Inventory on-hand + up-coming order) at the end of <em>t</em>-th day, and DoW_t is a 6-dimensional one-hot encoding form of identifying the day of week in which the state is in. This way, the ordering decisions can be made based on the information of which day of the week it is in.</p> <p>Action -&gt; (A_t), where A_t denotes the order quantity being released at the end of <em>t</em>-th day, the action space is limited by the maximum order quantity determined by suppliers or transportation capacity</p> <p>Reward -&gt; (R_t) = (min(D_t, I_t) * P) - (I_t * H) - (I(A_t &gt; 0) * F) - (A_t * V), with the definitions being</p> <ol> <li>D_t is the demand that occurs during the daytime of the (t+1)-th day,</li> <li>I_t is the inventory on-hand at the end of <em>t</em>-th day,</li> <li>P is the selling price per product unit,</li> <li>H is the holding cost per inventory on-hand per night,</li> <li>I(A_t &gt; 0) is an indicator function that takes 1 if (A_t &gt; 0) and 0 otherwise,</li> <li>F is the fixed ordering cost incurred per order, and</li> <li>V is the variable ordering cost per unit.</li> </ol> <h3 id="solving-the-mdp">Solving the MDP</h3> <p>From the case above, the transition probability (P_t) is unknown. In real cases, one could choose to fit a demand distribution using historical demand data, try to infer the transition probabilities and then use a model-based RL technique to solve this problem.</p> <p>However, this could result in a huge gap between the simulated environment and real world as fitting a perfect demand distribution is very challenging (especially in this case where demand follows a mixture of distributions). Hence, it would be better to adopt model-free RL techniques that can deal with unknown transition probabilities inherently.</p> <p>There are multiple model-free RL techniques for solving this MDP. In this article, as a first attempt, I adopted Deep Q Network (DQN) as the solving tool. DQN is a variant of Q learning, which utilizes deep neural networks to build an approximation of Q functions. To save on space, I’m omitting the detailed instruction of DQN as it’s not the focus of this article. Interested readers are referred to this <a href="https://unnatsingh.medium.com/deep-q-network-with-pytorch-d1ca6f40bfda" rel="external nofollow noopener" target="_blank">article</a>.</p> <h2 id="the-case">The Case</h2> <p>Assume there is a small retail store which sells Coke to its customers. Every time the store wants to replenish its inventory to fulfill customer demand, the store has to place an order of an integer number of cases of Coke (one case contains 24 cans). Suppose that the unit selling price of Coke is 30 dollars per case, holding cost is 3 dollars per case per night, fixed ordering cost is 50 dollars per order, variable ordering cost is 10 dollars per case, the inventory capacity of the store is 50 cases, the maximum order quantity allowed is 20 cases per order, the initial inventory is 25 cases at the end of a Sunday, and the lead time (time interval between placing an order and order arrival) is 2 days. Here, we assume the demand from Monday to Thursday follows a normal distribution N(3,1.5), the demand on Friday follows a normal distribution N(6,1), and the demand from Saturday to Sunday follows a normal distribution N(12,2). We generate 52 weeks of historical demand samples from this mixture of distributions, and use this as a training dataset for the DQN model.</p> <p>Summary:</p> <ol> <li>1 case = 24 cans of Coke</li> <li>Unit selling price (P) = 30 dollars per case</li> <li>Holding cost (H) = 3 dollars per case</li> <li>Fixed ordering cost (F) = 50 dollars per order</li> <li>Variable ordering cost (V) = 10 dollars per case</li> <li>Maximum inventory capacity (C) = 50 cases</li> <li>Maximum order quantity = 20 cases per order</li> <li>Initial inventory on-hand (I_0) = 25 cases at the end of Sunday</li> <li>Lead time (L) (Time between releasing an order and receiving said order) = 2 days</li> <li>The demand follows a normal distribution N(miu, sigma) with miu the mean and sigma being the standard deviation of the demand. Demand from Monday to Thursday follows a normal distribution N(3,1.5), the demand on Friday follows a normal distribution N(6,1), and the demand from Saturday to Sunday follows a normal distribution N(12,2)</li> <li>We generate 52 weeks of historical demand samples for training the DQN Model</li> </ol> <p>As a benchmark, we will optimize a classic (s,S) inventory control policy using the same dataset which was used for training the DQN model, and compare its performance with DQN in a test set.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Data Generation
</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">generate_demand</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">stdev</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">

    :param mean: mean of the normal distribution
    :param stdev: standard deviation of the normal distribution
    :return: demand value
    </span><span class="sh">"""</span>
    <span class="n">random_demand</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">stdev</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">random_demand</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">random_demand</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">random_demand</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">random_demand</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">random_demand</span>

<span class="n">demand_hist</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># List of demand data in the past year
</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">52</span><span class="p">):</span> <span class="c1"># 52 weeks in a year
</span>    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span> <span class="c1"># Data for monday through thursday
</span>        <span class="n">random_demand</span> <span class="o">=</span> <span class="nf">generate_demand</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mf">1.5</span><span class="p">)</span>
        <span class="n">demand_hist</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">random_demand</span><span class="p">)</span>
    <span class="n">random_demand</span> <span class="o">=</span> <span class="nf">generate_demand</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># Data for friday
</span>    <span class="n">demand_hist</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">random_demand</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span> <span class="c1"># Data for saturday and sunday
</span>        <span class="n">random_demand</span> <span class="o">=</span> <span class="nf">generate_demand</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">demand_hist</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">random_demand</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">demand_hist</span><span class="p">)</span>

<span class="n">plt</span><span class="p">.</span><span class="nf">hist</span><span class="p">(</span><span class="n">demand_hist</span><span class="p">)</span>
</code></pre></div></div> <h2 id="inventory-optimization-environment">Inventory Optimization Environment</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">InvOptEnv</span><span class="p">():</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">demand_records</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">n_period</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">demand_records</span><span class="p">)</span> <span class="c1"># Number of periods
</span>        <span class="n">self</span><span class="p">.</span><span class="n">current_period</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Intialized on Monday
</span>        <span class="n">self</span><span class="p">.</span><span class="n">day_of_week</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Intialized on Monday
</span>        <span class="n">self</span><span class="p">.</span><span class="n">inv_level</span> <span class="o">=</span> <span class="mi">25</span> <span class="c1"># Initial I_t (I_0)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">inv_pos</span> <span class="o">=</span> <span class="mi">25</span> <span class="c1"># Initial I_pt (I_0)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">capacity</span> <span class="o">=</span> <span class="mi">50</span> <span class="c1"># C value
</span>        <span class="n">self</span><span class="p">.</span><span class="n">holding_cost</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># H value
</span>        <span class="n">self</span><span class="p">.</span><span class="n">unit_price</span> <span class="o">=</span> <span class="mi">30</span> <span class="c1"># P value
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fixed_order_cost</span> <span class="o">=</span> <span class="mi">50</span> <span class="c1"># F value
</span>        <span class="n">self</span><span class="p">.</span><span class="n">variable_order_cost</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># V value
</span>        <span class="n">self</span><span class="p">.</span><span class="n">lead_time</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># L value
</span>        <span class="n">self</span><span class="p">.</span><span class="n">order_arrival_list</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># List of orders that are arriving in the future
</span>        <span class="n">self</span><span class="p">.</span><span class="n">demand_list</span> <span class="o">=</span> <span class="n">demand_records</span> <span class="c1"># List of demand data
</span>        <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">inv_pos</span><span class="p">]</span> <span class="o">+</span> <span class="nf">convert_day_of_week</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">day_of_week</span><span class="p">))</span> <span class="c1"># State vector
</span>        <span class="n">self</span><span class="p">.</span><span class="n">state_list</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># List of state vectors
</span>        <span class="n">self</span><span class="p">.</span><span class="n">state_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">state</span><span class="p">)</span> <span class="c1"># Add the initial state vector to the list
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_list</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># List of actions
</span>        <span class="n">self</span><span class="p">.</span><span class="n">reward_list</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># List of rewards
</span>
    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="n">self</span><span class="p">):</span> <span class="c1"># Reset the environment
</span>        <span class="n">self</span><span class="p">.</span><span class="n">state_list</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># Reset the state list
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_list</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># Reset the action list
</span>        <span class="n">self</span><span class="p">.</span><span class="n">reward_list</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># Reset the reward list
</span>        <span class="n">self</span><span class="p">.</span><span class="n">inv_level</span> <span class="o">=</span> <span class="mi">25</span> <span class="c1"># Reset the inventory level
</span>        <span class="n">self</span><span class="p">.</span><span class="n">inv_pos</span> <span class="o">=</span> <span class="mi">25</span> <span class="c1"># Reset the inventory position
</span>        <span class="n">self</span><span class="p">.</span><span class="n">current_period</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1"># Reset the current period
</span>        <span class="n">self</span><span class="p">.</span><span class="n">day_of_week</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Reset the day of week
</span>        <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">inv_pos</span><span class="p">]</span> <span class="o">+</span> <span class="nf">convert_day_of_week</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">day_of_week</span><span class="p">))</span> <span class="c1"># Reset the state vector
</span>        <span class="n">self</span><span class="p">.</span><span class="n">state_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">state</span><span class="p">)</span> <span class="c1"># Add the initial state vector to the list
</span>        <span class="n">self</span><span class="p">.</span><span class="n">order_arrival_list</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># Reset the order arrival list
</span>        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">state</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action</span><span class="p">):</span> <span class="c1"># Take an action
</span>        <span class="k">if</span> <span class="n">action</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># If the action is to order
</span>            <span class="n">y</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="n">self</span><span class="p">.</span><span class="n">order_arrival_list</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">current_period</span><span class="o">+</span><span class="n">self</span><span class="p">.</span><span class="n">lead_time</span><span class="p">,</span> <span class="n">action</span><span class="p">])</span> <span class="c1"># Add the order to the list
</span>        <span class="k">else</span><span class="p">:</span> <span class="c1"># If the action is not to order
</span>            <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">order_arrival_list</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># If there is an order in the list
</span>            <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_period</span> <span class="o">==</span> <span class="n">self</span><span class="p">.</span><span class="n">order_arrival_list</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span> <span class="c1"># If the order arrives
</span>                <span class="n">self</span><span class="p">.</span><span class="n">inv_level</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">capacity</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">inv_level</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">order_arrival_list</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># Update the inventory level
</span>                <span class="n">self</span><span class="p">.</span><span class="n">order_arrival_list</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="c1"># Remove the order from the list
</span>
        <span class="n">demand</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">demand_list</span><span class="p">[</span><span class="n">self</span><span class="p">.</span><span class="n">current_period</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Get the demand of the current period
</span>        <span class="n">units_sold</span> <span class="o">=</span> <span class="n">demand</span> <span class="k">if</span> <span class="n">demand</span> <span class="o">&lt;=</span> <span class="n">self</span><span class="p">.</span><span class="n">inv_level</span> <span class="k">else</span> <span class="n">self</span><span class="p">.</span><span class="n">inv_level</span> <span class="c1"># Calculate the number of units sold
</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="p">(</span><span class="n">units_sold</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">unit_price</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">inv_level</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">holding_cost</span><span class="p">)</span> \
                 <span class="o">-</span> <span class="p">(</span><span class="n">y</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">fixed_order_cost</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="n">action</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">variable_order_cost</span><span class="p">)</span> <span class="c1"># Calculate the reward
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">inv_level</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">inv_level</span> <span class="o">-</span> <span class="n">units_sold</span> <span class="c1"># Update the inventory level
</span>        <span class="n">self</span><span class="p">.</span><span class="n">inv_pos</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">inv_level</span> <span class="c1"># Update the inventory position
</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">order_arrival_list</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># If there is an order in the list
</span>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">order_arrival_list</span><span class="p">)):</span> <span class="c1"># For each order in the list
</span>                <span class="n">self</span><span class="p">.</span><span class="n">inv_pos</span> <span class="o">+=</span> <span class="n">self</span><span class="p">.</span><span class="n">order_arrival_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># Update the inventory position
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">day_of_week</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">day_of_week</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="mi">7</span> <span class="c1"># Update the day of week
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([</span><span class="n">self</span><span class="p">.</span><span class="n">inv_pos</span><span class="p">]</span> <span class="o">+</span> <span class="nf">convert_day_of_week</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">day_of_week</span><span class="p">))</span> <span class="c1"># Update the state vector
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">state_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">state</span><span class="p">)</span> <span class="c1"># Add the state vector to the list
</span>        <span class="n">self</span><span class="p">.</span><span class="n">action_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="c1"># Add the action to the list
</span>        <span class="n">self</span><span class="p">.</span><span class="n">reward_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span> <span class="c1"># Add the reward to the list
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">current_period</span> <span class="o">+=</span> <span class="mi">1</span> <span class="c1"># Update the current period
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">current_period</span> <span class="o">&gt;</span> <span class="n">self</span><span class="p">.</span><span class="n">n_period</span><span class="p">:</span> <span class="c1"># If the current period exceeds the total number of periods
</span>            <span class="n">done</span> <span class="o">=</span> <span class="bp">True</span> <span class="c1"># Done
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="n">done</span> <span class="o">=</span> <span class="bp">False</span> <span class="c1"># Not done
</span>
        <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span>

<span class="k">def</span> <span class="nf">convert_day_of_week</span><span class="p">(</span><span class="n">d</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">d</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># Monday
</span>        <span class="k">return</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">d</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">d</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">d</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">d</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">d</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">d</span> <span class="o">==</span> <span class="mi">6</span><span class="p">:</span>
        <span class="k">return</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">DQN</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Actor (Policy) Model
    </span><span class="sh">"""</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="n">fc1_dim</span><span class="p">,</span> <span class="n">fc2_dim</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initialize parameters and build model.
        =======
        :param state_dim: state dimension
        :param action_dim: action dimension
        :param fc1_dim: first fully connected layer dimension
        :param fc2_dim: second fully connected layer dimension
        </span><span class="sh">"""</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">DQN</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span> <span class="c1"># Inherit from the nn.Module class
</span>        <span class="n">self</span><span class="p">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span> <span class="c1"># Set the seed
</span>        <span class="n">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">fc1_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">fc1_dim</span><span class="p">,</span> <span class="n">fc2_dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">fc2_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Build a forward pass of the network from state -&gt; action

        :param x: state vector
        :return: final output
        </span><span class="sh">"""</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">relu</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="nf">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">fc3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="n">collections</span> <span class="kn">import</span> <span class="n">namedtuple</span><span class="p">,</span> <span class="n">deque</span>

<span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="n">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>

<span class="n">BUFFER_SIZE</span> <span class="o">=</span> <span class="nf">int</span><span class="p">(</span><span class="mi">5</span><span class="o">*</span><span class="mf">1e5</span><span class="p">)</span>  <span class="c1"># replay buffer size
</span><span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">128</span>         <span class="c1"># minibatch size
</span><span class="n">GAMMA</span> <span class="o">=</span> <span class="mf">0.99</span>            <span class="c1"># discount factor
</span><span class="n">TAU</span> <span class="o">=</span> <span class="mf">1e-3</span>              <span class="c1"># for soft update of target parameters
</span><span class="n">LR</span> <span class="o">=</span> <span class="mf">1e-4</span>               <span class="c1"># learning rate
</span><span class="n">UPDATE_EVERY</span> <span class="o">=</span> <span class="mi">4</span>        <span class="c1"># how often to update the network
</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">device</span><span class="p">(</span><span class="sh">"</span><span class="s">cuda:0</span><span class="sh">"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="sh">"</span><span class="s">cpu</span><span class="sh">"</span><span class="p">)</span> <span class="c1"># Use GPU if available
</span>
<span class="k">class</span> <span class="nc">Agent</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    Interacts with and learns from the environment.
    </span><span class="sh">"""</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initialize an Agent object.

        Params
        ======
            state_dim (int): dimension of each state
            action_dim (int): dimension of each action
            seed (int): random seed
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">state_dim</span> <span class="o">=</span> <span class="n">state_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_dim</span> <span class="o">=</span> <span class="n">action_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="c1"># DQN initialization
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dqn_local</span> <span class="o">=</span> <span class="nc">DQN</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># What does local DQN mean? -&gt; The DQN that is being trained
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dqn_target</span> <span class="o">=</span> <span class="nc">DQN</span><span class="p">(</span><span class="n">state_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">seed</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># What does target DQN mean? -&gt; The DQN that is being used to predict the Q values
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="nc">Adam</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">dqn_local</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">LR</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nc">ReplayBuffer</span><span class="p">(</span><span class="n">action_dim</span><span class="p">,</span> <span class="n">BUFFER_SIZE</span><span class="p">,</span> <span class="n">BATCH_SIZE</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span>

        <span class="n">self</span><span class="p">.</span><span class="n">t_step</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Initialize time step (for updating every UPDATE_EVERY step)
</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">Agent seed:</span><span class="sh">"</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_step</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Save experience in replay memory, and use random sample from buffer to learn.

        :param state: state vector
        :param action: action vector
        :param reward: reward vector
        :param next_step: next state vector
        :param done: done vector
        :return: None
        </span><span class="sh">"""</span>
        <span class="c1"># Save experience in replay memory
</span>        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_step</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>

        <span class="c1"># Learn every UPDATE_EVERY time steps.
</span>        <span class="n">self</span><span class="p">.</span><span class="n">t_step</span> <span class="o">=</span> <span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">t_step</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">UPDATE_EVERY</span>
        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">t_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># If it's time to learn
</span>            <span class="c1"># If enough samples are available in memory, get random subset and learn
</span>            <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">BATCH_SIZE</span><span class="p">:</span>
                <span class="n">experiences</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">sample</span><span class="p">()</span> <span class="c1"># Sample from the replay buffer
</span>                <span class="n">self</span><span class="p">.</span><span class="nf">learn</span><span class="p">(</span><span class="n">experiences</span><span class="p">,</span> <span class="n">GAMMA</span><span class="p">)</span> <span class="c1"># Learn from the sampled experiences
</span>
    <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Returns actions for given state as per current policy.

        :param state: current state
        :param eps: epsilon, for epsilon-greedy action selection
        :return: action vector
        </span><span class="sh">"""</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="p">).</span><span class="nf">float</span><span class="p">().</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># Convert the state vector to a torch tensor
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dqn_local</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span> <span class="c1"># Set the local DQN to evaluation mode
</span>        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span> <span class="c1"># Disable gradient calculation
</span>            <span class="n">action_values</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dqn_local</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="c1"># Get the action values
</span>        <span class="n">self</span><span class="p">.</span><span class="n">dqn_local</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span> <span class="c1"># Set the local DQN to training mode
</span>
        <span class="c1"># Epsilon-greedy action selection
</span>        <span class="k">if</span> <span class="n">random</span><span class="p">.</span><span class="nf">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">eps</span><span class="p">:</span> <span class="c1"># If the random number is greater than epsilon
</span>            <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">action_values</span><span class="p">.</span><span class="nf">cpu</span><span class="p">().</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span> <span class="c1"># Return the action with the highest action value
</span>        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">random</span><span class="p">.</span><span class="nf">choice</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">action_dim</span><span class="p">))</span> <span class="c1"># Return a random action
</span>
    <span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">experiences</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Update value parameters using given batch of experience tuples.

        :param self:
        :param experiences: (Tuple[torch.Variable]): tuple of (s, a, r, s</span><span class="sh">'</span><span class="s">, done) tuples
        :param gamma: discount factor
        :return:
        </span><span class="sh">"""</span>

        <span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">dones</span> <span class="o">=</span> <span class="n">experiences</span> <span class="c1"># Unpack the experiences
</span>
        <span class="c1">## Compute and minimize the loss
</span>        <span class="n">criterion</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">MSELoss</span><span class="p">()</span> <span class="c1"># Define the loss function
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dqn_local</span><span class="p">.</span><span class="nf">train</span><span class="p">()</span> <span class="c1"># Set the local DQN to training mode
</span>
        <span class="n">self</span><span class="p">.</span><span class="n">dqn_target</span><span class="p">.</span><span class="nf">eval</span><span class="p">()</span> <span class="c1"># Set the target DQN to evaluation mode
</span>
        <span class="c1"># Get the predicted Q values from the local DQN
</span>        <span class="n">predicted_targets</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">dqn_local</span><span class="p">(</span><span class="n">states</span><span class="p">).</span><span class="nf">gather</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">actions</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="nf">no_grad</span><span class="p">():</span> <span class="c1"># Disable gradient calculation
</span>            <span class="c1"># Get the target Q values from the target DQN
</span>            <span class="n">labels_next</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="nf">dqn_target</span><span class="p">(</span><span class="n">next_states</span><span class="p">).</span><span class="nf">detach</span><span class="p">().</span><span class="nf">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
            <span class="c1"># .detach() -&gt;  Returns a new Tensor, detached from the current graph.
</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">+</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">*</span> <span class="n">labels_next</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">dones</span><span class="p">))</span> <span class="c1"># Calculate the labels
</span>        <span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">predicted_targets</span><span class="p">,</span> <span class="n">labels</span><span class="p">).</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># Calculate the loss
</span>        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">zero_grad</span><span class="p">()</span> <span class="c1"># Zero the gradients
</span>        <span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span> <span class="c1"># Backpropagate
</span>        <span class="n">self</span><span class="p">.</span><span class="n">optimizer</span><span class="p">.</span><span class="nf">step</span><span class="p">()</span> <span class="c1"># Update the weights
</span>
        <span class="c1"># ------------------- update target network ------------------- #
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">soft_update</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">dqn_local</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">dqn_target</span><span class="p">,</span> <span class="n">TAU</span><span class="p">)</span> <span class="c1"># Soft update the target DQN
</span>

    <span class="k">def</span> <span class="nf">soft_update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">local_model</span><span class="p">,</span> <span class="n">target_model</span><span class="p">,</span> <span class="n">tau</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Soft update model parameters.

        θ_target = τ*θ_local + (1 - τ)*θ_target

        :param local_model: weights will be copied from
        :param target_model: weights will be copied to
        :param tau: interpolation parameter
        :return: None
        </span><span class="sh">"""</span>
        <span class="k">for</span> <span class="n">target_param</span><span class="p">,</span> <span class="n">local_param</span> <span class="ow">in</span> <span class="nf">zip</span><span class="p">(</span><span class="n">target_model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">(),</span> <span class="n">local_model</span><span class="p">.</span><span class="nf">parameters</span><span class="p">()):</span>
                <span class="n">target_param</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="nf">copy_</span><span class="p">(</span><span class="n">tau</span><span class="o">*</span><span class="n">local_param</span><span class="p">.</span><span class="n">data</span> <span class="o">+</span> <span class="p">(</span><span class="mf">1.0</span><span class="o">-</span><span class="n">tau</span><span class="p">)</span><span class="o">*</span><span class="n">target_param</span><span class="p">.</span><span class="n">data</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">ReplayBuffer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">buffer_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Initialize a ReplayBuffer object.

        :param action_dim: action dimension
        :param buffer_size: maximum size of buffer
        :param batch_size: size of each training batch
        :param seed: random seed
        </span><span class="sh">"""</span>
        <span class="n">self</span><span class="p">.</span><span class="n">action_dim</span> <span class="o">=</span> <span class="n">action_dim</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span> <span class="o">=</span> <span class="nf">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="n">buffer_size</span><span class="p">)</span> <span class="c1"># Initialize the replay buffer
</span>        <span class="n">self</span><span class="p">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
        <span class="n">self</span><span class="p">.</span><span class="n">experience</span> <span class="o">=</span> <span class="nf">namedtuple</span><span class="p">(</span><span class="sh">"</span><span class="s">Experience</span><span class="sh">"</span><span class="p">,</span> <span class="n">field_names</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">state</span><span class="sh">"</span><span class="p">,</span>
                                                                    <span class="sh">"</span><span class="s">action</span><span class="sh">"</span><span class="p">,</span>
                                                                    <span class="sh">"</span><span class="s">reward</span><span class="sh">"</span><span class="p">,</span>
                                                                    <span class="sh">"</span><span class="s">next_state</span><span class="sh">"</span><span class="p">,</span>
                                                                    <span class="sh">"</span><span class="s">done</span><span class="sh">"</span><span class="p">])</span>
        <span class="n">self</span><span class="p">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">ReplayBuffer seed:</span><span class="sh">"</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Add a new experience to memory.
        </span><span class="sh">"""</span>
        <span class="n">e</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">experience</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">action</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">next_state</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Randomly sample a batch of experiences from memory.
        </span><span class="sh">"""</span>
        <span class="n">experiences</span> <span class="o">=</span> <span class="n">random</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">batch_size</span><span class="p">)</span>

        <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">e</span><span class="p">.</span><span class="n">state</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">experiences</span> <span class="k">if</span> <span class="n">e</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">])).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">actions</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">e</span><span class="p">.</span><span class="n">action</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">experiences</span> <span class="k">if</span> <span class="n">e</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">])).</span><span class="nf">long</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">rewards</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">e</span><span class="p">.</span><span class="n">reward</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">experiences</span> <span class="k">if</span> <span class="n">e</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">])).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">next_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">e</span><span class="p">.</span><span class="n">next_state</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">experiences</span> <span class="k">if</span> <span class="n">e</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">])).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">dones</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">from_numpy</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nf">vstack</span><span class="p">([</span><span class="n">e</span><span class="p">.</span><span class="n">done</span> <span class="k">for</span> <span class="n">e</span> <span class="ow">in</span> <span class="n">experiences</span> <span class="k">if</span> <span class="n">e</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">]).</span><span class="nf">astype</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">uint8</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

        <span class="nf">return </span><span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_states</span><span class="p">,</span> <span class="n">dones</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">__len__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">
        Return the current size of internal memory.
        </span><span class="sh">"""</span>
        <span class="k">return</span> <span class="nf">len</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">memory</span><span class="p">)</span>
</code></pre></div></div> <h2 id="training-the-dqn-model">Training the DQN Model</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">torch.utils.tensorboard</span> <span class="kn">import</span> <span class="n">SummaryWriter</span>

<span class="n">agent</span> <span class="o">=</span> <span class="nc">Agent</span><span class="p">(</span><span class="n">state_dim</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span><span class="n">action_dim</span><span class="o">=</span><span class="mi">21</span><span class="p">,</span><span class="n">seed</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">dqn</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="n">n_episodes</span><span class="o">=</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">max_t</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">,</span> <span class="n">eps_start</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">eps_end</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">,</span>
       <span class="n">eps_decay</span><span class="o">=</span><span class="mf">0.995</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Deep Q-Learning

    Params
    ======
        n_episodes (int): maximum number of training epsiodes
        max_t (int): maximum number of timesteps per episode
        eps_start (float): starting value of epsilon, for epsilon-greedy action selection
        eps_end (float): minimum value of epsilon
        eps_decay (float): mutiplicative factor (per episode) for decreasing epsilon

    </span><span class="sh">"""</span>

     <span class="c1">#Tensorboard setup
</span>    <span class="n">writer</span><span class="o">=</span><span class="nc">SummaryWriter</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">logs/dqn-v1-ref</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># list containing score from each episode
</span>    <span class="n">eps</span> <span class="o">=</span> <span class="n">eps_start</span>

    <span class="k">for</span> <span class="n">i_episode</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">reset</span><span class="p">()</span>
        <span class="n">score</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">max_t</span><span class="p">):</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">act</span><span class="p">(</span><span class="n">state</span><span class="p">,</span><span class="n">eps</span><span class="p">)</span>
            <span class="n">next_state</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">done</span> <span class="o">=</span> <span class="n">env</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            <span class="n">agent</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span><span class="n">action</span><span class="p">,</span><span class="n">reward</span><span class="p">,</span><span class="n">next_state</span><span class="p">,</span><span class="n">done</span><span class="p">)</span>

            <span class="n">state</span> <span class="o">=</span> <span class="n">next_state</span>
            <span class="n">score</span> <span class="o">+=</span> <span class="n">reward</span>

            <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
                <span class="nf">print</span><span class="p">(</span><span class="sh">'</span><span class="s">episode</span><span class="sh">'</span> <span class="o">+</span> <span class="nf">str</span><span class="p">(</span><span class="n">i_episode</span><span class="p">)</span> <span class="o">+</span> <span class="sh">'</span><span class="s">:</span><span class="sh">'</span><span class="p">,</span> <span class="n">score</span><span class="p">)</span>
                <span class="n">scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>

                <span class="c1"># Log the score to TensorBoard
</span>                <span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">logs/Score/</span><span class="sh">"</span><span class="p">,</span> <span class="n">score</span><span class="p">,</span> <span class="n">i_episode</span><span class="p">)</span>

                <span class="c1"># Log the average score over the last 100 episodes
</span>                <span class="n">avg_score</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">mean</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="o">-</span><span class="mi">100</span><span class="p">:])</span>
                <span class="n">writer</span><span class="p">.</span><span class="nf">add_scalar</span><span class="p">(</span><span class="sa">r</span><span class="sh">"</span><span class="s">logs/Average_Score/</span><span class="sh">"</span><span class="p">,</span> <span class="n">avg_score</span><span class="p">,</span> <span class="n">i_episode</span><span class="p">)</span>
                <span class="k">break</span>

        <span class="n">eps</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">eps</span><span class="o">*</span><span class="n">eps_decay</span><span class="p">,</span><span class="n">eps_end</span><span class="p">)</span><span class="c1">## decrease the epsilon
</span>    <span class="k">return</span> <span class="n">scores</span>

<span class="n">env</span> <span class="o">=</span> <span class="nc">InvOptEnv</span><span class="p">(</span><span class="n">demand_hist</span><span class="p">)</span>
<span class="n">scores</span><span class="o">=</span> <span class="nf">dqn</span><span class="p">(</span><span class="n">env</span><span class="p">)</span>
</code></pre></div></div> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rl-inventory-optimization-results-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rl-inventory-optimization-results-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rl-inventory-optimization-results-1400.webp"></source> <img src="/assets/img/rl-inventory-optimization-results.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Tensorboard log of each iteration of the model done by Ryan. </div> <h2 id="optimizing-the-s-s-policy">Optimizing the (s, S) Policy</h2> <p>Both s and S are discrete values meaning there is a limited number of possible (s, S) policy combinations. We can use a brute force method to find the optimal (s, S) policy by iterating through all possible combinations and find the one that maximizes the profit.</p> <p>However, this method is computationally expensive and not scalable. Hence, we will use a heuristic method to find the optimal (s, S) policy.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">profit_calculation_sS</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">demand_records</span><span class="p">):</span>
    <span class="n">total_profit</span> <span class="o">=</span> <span class="mi">0</span> <span class="c1"># Initialize the total profit
</span>    <span class="n">inv_level</span> <span class="o">=</span> <span class="mi">25</span> <span class="c1"># inventory on hand, use this to calculate inventory costs
</span>    <span class="n">lead_time</span> <span class="o">=</span> <span class="mi">2</span>
    <span class="n">capacity</span> <span class="o">=</span> <span class="mi">50</span> <span class="c1"># Maximum inventory capacity
</span>    <span class="n">holding_cost</span> <span class="o">=</span> <span class="mi">3</span> <span class="c1"># holding cost per unit per night
</span>    <span class="n">fixed_order_cost</span> <span class="o">=</span> <span class="mi">50</span> <span class="c1"># fixed ordering cost per order
</span>    <span class="n">variable_order_cost</span> <span class="o">=</span> <span class="mi">10</span> <span class="c1"># variable ordering cost per unit
</span>    <span class="n">unit_price</span> <span class="o">=</span> <span class="mi">30</span> <span class="c1"># selling price per unit
</span>    <span class="n">order_arrival_list</span> <span class="o">=</span> <span class="p">[]</span> <span class="c1"># list of orders that are arriving in the future
</span>
    <span class="k">for</span> <span class="n">current_period</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">demand_records</span><span class="p">)):</span>
        <span class="n">inv_pos</span> <span class="o">=</span> <span class="n">inv_level</span> <span class="c1"># inventory position, use this to determine whether to order
</span>
        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">order_arrival_list</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span> <span class="c1"># if there is an order in the list
</span>            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">order_arrival_list</span><span class="p">)):</span> <span class="c1"># for each order in the list
</span>                <span class="n">inv_pos</span> <span class="o">+=</span> <span class="n">order_arrival_list</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># update the inventory position
</span>
        <span class="k">if</span> <span class="n">inv_pos</span> <span class="o">&lt;=</span> <span class="n">s</span><span class="p">:</span> <span class="c1"># if the inventory position is less than or equal to s
</span>            <span class="n">order_quantity</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="n">S</span><span class="o">-</span><span class="n">inv_pos</span><span class="p">)</span> <span class="c1"># order up to S units, with maximum of 20 units (maximum order capacity)
</span>            <span class="n">order_arrival_list</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">current_period</span><span class="o">+</span><span class="n">lead_time</span><span class="p">,</span> <span class="n">order_quantity</span><span class="p">])</span> <span class="c1"># add the order to the list
</span>            <span class="n">y</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">order_quantity</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">y</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">order_arrival_list</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">current_period</span> <span class="o">==</span> <span class="n">order_arrival_list</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]:</span>
                <span class="n">inv_level</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">capacity</span><span class="p">,</span> <span class="n">inv_level</span> <span class="o">+</span> <span class="n">order_arrival_list</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">])</span>
                <span class="n">order_arrival_list</span><span class="p">.</span><span class="nf">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

        <span class="n">demand</span> <span class="o">=</span> <span class="n">demand_records</span><span class="p">[</span><span class="n">current_period</span><span class="p">]</span>
        <span class="n">units_sold</span> <span class="o">=</span> <span class="n">demand</span> <span class="k">if</span> <span class="n">demand</span> <span class="o">&lt;=</span> <span class="n">inv_level</span> <span class="k">else</span> <span class="n">inv_level</span>
        <span class="n">profit</span> <span class="o">=</span> <span class="n">units_sold</span><span class="o">*</span><span class="n">unit_price</span><span class="o">-</span><span class="n">holding_cost</span><span class="o">*</span><span class="n">inv_level</span><span class="o">-</span><span class="n">y</span><span class="o">*</span><span class="n">fixed_order_cost</span><span class="o">-</span><span class="n">order_quantity</span><span class="o">*</span><span class="n">variable_order_cost</span>
        <span class="n">inv_level</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">inv_level</span><span class="o">-</span><span class="n">demand</span><span class="p">)</span>
        <span class="n">total_profit</span> <span class="o">+=</span> <span class="n">profit</span>

    <span class="k">return</span> <span class="n">total_profit</span>

<span class="n">s_S_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">S</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">61</span><span class="p">):</span> <span class="c1"># give a little room to allow S to exceed the capacity, should be calculated using safety stock
</span>    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="n">S</span><span class="p">):</span>
        <span class="n">s_S_list</span><span class="p">.</span><span class="nf">append</span><span class="p">([</span><span class="n">s</span><span class="p">,</span><span class="n">S</span><span class="p">])</span>

<span class="n">profit_sS_list</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sS</span> <span class="ow">in</span> <span class="n">s_S_list</span><span class="p">:</span>
    <span class="n">profit_sS_list</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="nf">profit_calculation_sS</span><span class="p">(</span><span class="n">sS</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">sS</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="n">demand_hist</span><span class="p">))</span>

<span class="n">best_sS_profit</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">profit_sS_list</span><span class="p">)</span>
<span class="n">best_sS</span> <span class="o">=</span> <span class="n">s_S_list</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">profit_sS_list</span><span class="p">)]</span>
</code></pre></div></div> <h2 id="evaluating-the-model-on-test-data">Evaluating the model on test data</h2> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">demand_test</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span><span class="mi">200</span><span class="p">):</span>
    <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">seed</span><span class="p">(</span><span class="n">k</span><span class="p">)</span>
    <span class="n">demand_future</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">52</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="n">random_demand</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">random_demand</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">random_demand</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">random_demand</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">random_demand</span><span class="p">)</span>
            <span class="n">demand_future</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">random_demand</span><span class="p">)</span>
        <span class="n">random_demand</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">random_demand</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">random_demand</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">random_demand</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">random_demand</span><span class="p">)</span>
        <span class="n">demand_future</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">random_demand</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
            <span class="n">random_demand</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="nf">normal</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">random_demand</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">random_demand</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">random_demand</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">random_demand</span><span class="p">)</span>
            <span class="n">demand_future</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">random_demand</span><span class="p">)</span>
    <span class="n">demand_test</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">demand_future</span><span class="p">)</span>
</code></pre></div></div> <p>The results show that in 100 different scenarios of a year’s worth of demand data, our agent that has been trained with Deep Q-Network performs better than the classic (s, S) Policy</p> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rl-inventory-optimization-thumbnail-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rl-inventory-optimization-thumbnail-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rl-inventory-optimization-thumbnail-1400.webp"></source> <img src="/assets/img/rl-inventory-optimization-thumbnail.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="example image" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> The Deep Q-Network agent performing better than the classic (s, S) policy. </div> <p>Now, let’s dive even deeper towards the specific actions that the agent took differently compared to the (s, S) policy.</p> <p>This project is heavily inspired by the post made by Guangrui Xie, check it out <a href="https://medium.com/towards-data-science/a-reinforcement-learning-based-inventory-control-policy-for-retailers-ac35bc592278" rel="external nofollow noopener" target="_blank">here!</a></p> </div> </article><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;"> <script>let giscusTheme=localStorage.getItem("theme"),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ryanrahmadifa/ryanrahmadifa.github.io","data-repo-id":"R_kgDOK9L6YA","data-category":"General","data-category-id":"DIC_kwDOK9L6YM4CcEm1","data-mapping":"pathname","data-strict":"0","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":"preferred_color_scheme","data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,a])=>giscusScript.setAttribute(t,a)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Muhammad Ryanrahmadifa (Ryan). Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Last updated: December 30, 2023. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?7b30caa5023af4af8408a472dc4e1ebb"></script> <script defer src="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?07b8786bab9b4abe90d10e61f7d12ff7" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>